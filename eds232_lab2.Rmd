---
title: "Lab 2 - Community"
author: "Alex Vand"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Clustering

## k-means Clustering

```{r}
# load R packages
librarian::shelf(
  dplyr, DT, ggplot2, palmerpenguins, skimr, tibble)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
```


```{r}
# skim the table for a summary
skim(penguins)
```


```{r}
# remove the rows with NAs
penguins <- na.omit(penguins)

# plot petal length vs width, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```


```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# show cluster result
penguins_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```
**Question**: The `kmeans` clusters are most different from the observed species plot for penguins with bill length greater than 40-45 mm, and especially for penguins with bill depth greater than 15 mm. The `kmeans` clusters are similar to the observed species plot for most of the Adelie species (shown in red), however there is a significant number of incorrect classifications between smaller Chinstraps and larger Gentoo penguins (shown in green and blue).

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos
```


```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f=0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f=0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")

```

**k = 2 **

```{r}
# cluster using kmeans
k <- 2  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

**k = 8 **

```{r}
# cluster using kmeans
k <- 8  # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, 
    outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

## Hierarchical Clustering

```{r}
librarian::shelf(
  cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Question**: The columns of the `dune` data frame list 30 different species and the rows list the number of observations at 20 different sites.

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

**Question**: Bray Curtis distance is a measure between 0 and 1 (0 = identical, 1 = most different), whereas Euclidean distance is the hypotenuse. Bray Curtis distance is a better measure of species dissimilarity vs. Euclidean distance.


```{r}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

**Question**: First, the the dissimilarity values are calculated with `vegdist()` (here we use Bray Curtis dissimilarity). Next, these values are the input of `hclust()`, using the "complete" agglomeration method. We use `vegdist()` first because this function does the actual computing of the dissimilarity indices, while the `hclust()` executes a hierarchical cluster analysis on this set of given dissimilarities.


```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

**Question**: `hclust()` = hierarchical clustering, `agnes()` = agglomerative nesting

When the mean method of calculating the distance between observations and clusters is used, `hclust()` only uses the two observations and/or clusters which were recently merged when updating the distance matrix, while `agnes()` calculates those distances as the average of all the distances between all the observations in the two clusters.


```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

**Question**: Of the 4 methods, `ward` is the “best” model because the Agglomerative Coefficient is closest to 1 (~0.69).


```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```



```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question**: `agnes()` = agglomerative clustering, bottom-up, starts with each observation in its own cluster; `diana()` = divisive hierarchical clustering, top-down, starts with one big cluster.


```{r}
librarian::shelf(factoextra)

# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question**: The silhouette method has a higher optimal number of clusters (4) compared with the Gap Statistic method (3 clusters).


```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

**Question**: In dendrogram plots, the height of their shared connection (shown directly above on the x-axis) is the biggest determinant of relatedness between observations.


# Ordination

## Principal Components Analysis (PCA)

```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)

# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

```{r}
my_basket
```


```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```


**Question**: The `pca_method` of "GramSVD" is chosen because this data is mostly numerical. If the data contained categorical variables, "GLRM" should be used.


**Question**: There are `r ncol(my_basket.h2o)` columns, and therefore the same number of initial principal components (42).


```{r}
my_pca@model$model_summary %>% 
  add_rownames() %>% 
  tidyr::pivot_longer(-rowname) %>% 
  filter(
    rowname == "Proportion of Variance") %>% 
  mutate(
    pc = stringr::str_replace(name, "pc", "") %>% as.integer()) %>% 
  ggplot(aes(x = pc, y=cumsum(value))) +
  geom_point() + geom_line() +
  theme(axis.text.x = element_text(angle=90, hjust = 1))  +
  ylab("Cumulative Proportion of Variance Explained")
```

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```


**Question**: Beer and wine contributed most to PC1.


```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```


**Question**: Vegetables contributed the least to PC1 but positively towards PC2.



```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```

**Question**: In order to explain 90% of the total variance, `r min(which(ve$CVE >= 0.90))` principal components should be included.


```{r}
# Scree plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```


**Question**: There are 7 principal components to include up to the elbow of the PVE.


**Question**: Disadvantages of PCA include
(1) PCA can be highly affected by outliers and
(2) PCA does not perform as well in very high dimensional space where complex nonlinear patterns often exist.


## Non-metric MultiDimensional Scaling (NMDS)
 
```{r}
# load R packages
librarian::shelf(
  vegan, vegan3d)

# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

if (interactive()){
  help(varechem)
  help(varespec)
}

varespec %>% tibble()
```

**Question**: The `varespec`data frame has `r nrow(varespec)` rows (sites) and `r ncol(varespec)` columns (species).



```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

**Question**: The NMDS fit has an R^2 of 0.971, which is higher than the R^2 value for the linear fit (0.84) by about 0.07 (7%).


```{r}
ordiplot(vare.mds0, type = "t")
```


**Question**: Sites 5 and 8 are  most dissimilar based on species composition for the first component (MDS1). Sites 9/14 and 21 are most dissimilar based on MDS2.


```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```


```{r}
plot(vare.mds, type = "t")
```


**Question**: The `monoMDS` function runs NMDS by finding two dimensions and using random configuration as the starting solution. The solution iterates and the result is a long list including the final configuration and the stress statistic. The `metaMDS` function uses several random starts and selects among similar solutions with smallest stresses. Additionally, the result of `metaMDS` is more complicated and has more components.


```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```


```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```


**Question**: Aluminum (Al) and iron (Fe) have the strongest negative relationship with NMDS1 based on species composition.



```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca
         
         , data=varechem, add = TRUE, col = "green4")
```

**Question**: The highest values of Ca are found in the upper middle (NMDS1~0, NMDS2>0) and lower right (NMDS1>0, NMDS2<0); and the lowest values of Ca are found in the bottom left of the graph (NMDS1<0, NMDS2<0).  


```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```


**Question**: Within an ecological context, "constrained" ordination is determined by species and other environmental variables and "unconstrained" ordination is determined by species only ("unconstrained" by the environment).



```{r}
# plot ordination
plot(vare.cca)
```


**Question**: Sites 28 and 4 are most differentiated by CCA1. Aluminum (Al) is the strongest environmental vector for CCA1.



```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```


```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

